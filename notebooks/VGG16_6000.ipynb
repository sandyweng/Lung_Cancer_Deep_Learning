{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conscious-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hazardous-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir2 = '/Users/sandy/Desktop/Bigger Dataset'\n",
    "train_dir = os.path.join(base_dir2, 'train')\n",
    "validation_dir = os.path.join(base_dir2, 'validation')\n",
    "test_dir = os.path.join(base_dir2, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smaller-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": [
    "conv_base = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\", input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-championship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count,3))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "test_features, test_labels = extract_features(test_dir, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "committed-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 3 classes.\n",
      "Found 3000 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_features, validation_labels = extract_features(validation_dir, 1500)\n",
    "train_features, train_labels = extract_features(train_dir, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painted-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_reshaped=np.reshape(test_features, (1500, 4*4*512))\n",
    "train_features_reshaped = np.reshape(train_features, (3000, 4 * 4 * 512))\n",
    "validation_features_reshaped = np.reshape(validation_features, (1500, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instant-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "immediate-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 2s 8ms/step - loss: 0.6269 - acc: 0.7761 - val_loss: 0.2163 - val_acc: 0.9153\n",
      "INFO:tensorflow:Assets written to: /Users/sandy\\Desktop\\assets\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1966 - acc: 0.9271 - val_loss: 0.1514 - val_acc: 0.9373\n",
      "INFO:tensorflow:Assets written to: /Users/sandy\\Desktop\\assets\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1644 - acc: 0.9412 - val_loss: 0.1528 - val_acc: 0.9407\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1191 - acc: 0.9579 - val_loss: 0.1448 - val_acc: 0.9440\n",
      "INFO:tensorflow:Assets written to: /Users/sandy\\Desktop\\assets\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1136 - acc: 0.9531 - val_loss: 0.1492 - val_acc: 0.9340\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1016 - acc: 0.9616 - val_loss: 0.1070 - val_acc: 0.9607\n",
      "INFO:tensorflow:Assets written to: /Users/sandy\\Desktop\\assets\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1003 - acc: 0.9638 - val_loss: 0.1399 - val_acc: 0.9420\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1565 - acc: 0.9394 - val_loss: 0.1073 - val_acc: 0.9580\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0767 - acc: 0.9729 - val_loss: 0.1061 - val_acc: 0.9593\n",
      "INFO:tensorflow:Assets written to: /Users/sandy\\Desktop\\assets\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0948 - acc: 0.9667 - val_loss: 0.1077 - val_acc: 0.9600\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0934 - acc: 0.9616 - val_loss: 0.1191 - val_acc: 0.9533\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0839 - acc: 0.9658 - val_loss: 0.1017 - val_acc: 0.9633\n",
      "INFO:tensorflow:Assets written to: /Users/sandy\\Desktop\\assets\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0620 - acc: 0.9745 - val_loss: 0.1099 - val_acc: 0.9653\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0740 - acc: 0.9760 - val_loss: 0.1085 - val_acc: 0.9593\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0657 - acc: 0.9725 - val_loss: 0.1201 - val_acc: 0.9613\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0678 - acc: 0.9706 - val_loss: 0.1215 - val_acc: 0.9587\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0514 - acc: 0.9811 - val_loss: 0.1506 - val_acc: 0.9507\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0754 - acc: 0.9706 - val_loss: 0.1159 - val_acc: 0.9573\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0499 - acc: 0.9815 - val_loss: 0.1206 - val_acc: 0.9573\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0671 - acc: 0.9675 - val_loss: 0.1210 - val_acc: 0.9653\n"
     ]
    }
   ],
   "source": [
    "# feature extraction model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, InputLayer, Dropout\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model= models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features_reshaped, train_labels,\n",
    "                    epochs=100,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features_reshaped, validation_labels), \n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8),\n",
    "                    tf.keras.callbacks.ModelCheckpoint('/Users/sandy/Desktop', monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "contained-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the vgg base\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, InputLayer, Dropout\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interstate-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "atmospheric-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 3 classes.\n",
      "Found 1500 images belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 65s 3s/step - loss: 1.1543 - acc: 0.2689 - val_loss: 0.9761 - val_acc: 0.5307\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.9655 - acc: 0.5454 - val_loss: 0.9173 - val_acc: 0.6120\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.8840 - acc: 0.6735 - val_loss: 0.8394 - val_acc: 0.6760\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 72s 4s/step - loss: 0.7666 - acc: 0.7296 - val_loss: 0.7856 - val_acc: 0.6987\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 78s 4s/step - loss: 0.8747 - acc: 0.6131 - val_loss: 0.7542 - val_acc: 0.6993\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.6849 - acc: 0.7877 - val_loss: 0.7240 - val_acc: 0.7133\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 75s 4s/step - loss: 0.7115 - acc: 0.7639 - val_loss: 0.6742 - val_acc: 0.7700\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 71s 4s/step - loss: 0.5857 - acc: 0.8180 - val_loss: 0.6305 - val_acc: 0.7887\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.6585 - acc: 0.7878 - val_loss: 0.5869 - val_acc: 0.8460\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.5867 - acc: 0.8314 - val_loss: 0.5647 - val_acc: 0.8513\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.5933 - acc: 0.8214 - val_loss: 0.5492 - val_acc: 0.8487\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.4948 - acc: 0.8800 - val_loss: 0.5278 - val_acc: 0.8353\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.4484 - acc: 0.8639 - val_loss: 0.5082 - val_acc: 0.8187\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.3899 - acc: 0.8960 - val_loss: 0.5193 - val_acc: 0.7820\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.4816 - acc: 0.8376 - val_loss: 0.4632 - val_acc: 0.8447\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.4449 - acc: 0.8738 - val_loss: 0.4828 - val_acc: 0.8287\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.4578 - acc: 0.8712 - val_loss: 0.4511 - val_acc: 0.8320\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 65s 3s/step - loss: 0.4312 - acc: 0.8638 - val_loss: 0.4174 - val_acc: 0.8900\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.4004 - acc: 0.9097 - val_loss: 0.3960 - val_acc: 0.9060\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 66s 3s/step - loss: 0.3789 - acc: 0.8736 - val_loss: 0.3865 - val_acc: 0.9007\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.4572 - acc: 0.8608 - val_loss: 0.3781 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.3696 - acc: 0.9364 - val_loss: 0.3716 - val_acc: 0.8920\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.4066 - acc: 0.8690 - val_loss: 0.3660 - val_acc: 0.8900\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.3009 - acc: 0.9082 - val_loss: 0.3545 - val_acc: 0.8980\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.3929 - acc: 0.8917 - val_loss: 0.3393 - val_acc: 0.9093\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.3369 - acc: 0.9182 - val_loss: 0.3439 - val_acc: 0.8940\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 73s 4s/step - loss: 0.3457 - acc: 0.9027 - val_loss: 0.3244 - val_acc: 0.9107\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.3200 - acc: 0.8968 - val_loss: 0.3245 - val_acc: 0.9053\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 71s 4s/step - loss: 0.3187 - acc: 0.9158 - val_loss: 0.3114 - val_acc: 0.9160\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.3658 - acc: 0.9048 - val_loss: 0.3064 - val_acc: 0.9147\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.2611 - acc: 0.9283 - val_loss: 0.2988 - val_acc: 0.9187\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.2886 - acc: 0.9425 - val_loss: 0.3021 - val_acc: 0.9040\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 68s 4s/step - loss: 0.2982 - acc: 0.9227 - val_loss: 0.3189 - val_acc: 0.8947\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 67s 4s/step - loss: 0.3047 - acc: 0.9170 - val_loss: 0.2852 - val_acc: 0.9167\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.3214 - acc: 0.8939 - val_loss: 0.2814 - val_acc: 0.9193\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 72s 4s/step - loss: 0.2928 - acc: 0.9098 - val_loss: 0.2814 - val_acc: 0.9140\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.2055 - acc: 0.9366 - val_loss: 0.2868 - val_acc: 0.9033\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.2333 - acc: 0.9319 - val_loss: 0.2718 - val_acc: 0.9173\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.2420 - acc: 0.9522 - val_loss: 0.2824 - val_acc: 0.8993\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 69s 4s/step - loss: 0.2341 - acc: 0.9583 - val_loss: 0.2698 - val_acc: 0.9153\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 79s 4s/step - loss: 0.3068 - acc: 0.9057 - val_loss: 0.2558 - val_acc: 0.9287\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 81s 4s/step - loss: 0.2853 - acc: 0.9170 - val_loss: 0.2527 - val_acc: 0.9287\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.2436 - acc: 0.9422 - val_loss: 0.2736 - val_acc: 0.9060\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 76s 4s/step - loss: 0.2036 - acc: 0.9505 - val_loss: 0.2452 - val_acc: 0.9327\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.2282 - acc: 0.9501 - val_loss: 0.2599 - val_acc: 0.9107\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 75s 4s/step - loss: 0.2332 - acc: 0.9347 - val_loss: 0.2471 - val_acc: 0.9193\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 73s 4s/step - loss: 0.2600 - acc: 0.9128 - val_loss: 0.2385 - val_acc: 0.9280\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 73s 4s/step - loss: 0.1942 - acc: 0.9499 - val_loss: 0.2347 - val_acc: 0.9353\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.2119 - acc: 0.9433 - val_loss: 0.2351 - val_acc: 0.9260\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.1957 - acc: 0.9368 - val_loss: 0.2345 - val_acc: 0.9267\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 71s 4s/step - loss: 0.2350 - acc: 0.9133 - val_loss: 0.2360 - val_acc: 0.9233\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        \n",
    "        class_mode='categorical', shuffle=False)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical', shuffle=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=20,\n",
    "      epochs=100, validation_data=validation_generator, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "animated-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 3 classes.\n",
      "50/50 [==============================] - 36s 714ms/step - loss: 0.2549 - acc: 0.9180\n",
      "test acc: 0.9179999828338623\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20, shuffle=False\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "macro-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)\n",
    "\n",
    "predictions = model.predict(test_generator, steps=test_steps_per_epoch)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amino-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "willing-constant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.93      0.85      0.89       500\n",
      "      lung_n       0.94      0.99      0.96       500\n",
      "    lung_scc       0.91      0.95      0.93       500\n",
      "\n",
      "    accuracy                           0.93      1500\n",
      "   macro avg       0.93      0.93      0.93      1500\n",
      "weighted avg       0.93      0.93      0.93      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "shared-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "opposite-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[414  24  62]\n",
      " [  7 493   0]\n",
      " [ 22   0 478]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(validation_generator, test_generator.samples / test_generator.batch_size)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-publicity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
